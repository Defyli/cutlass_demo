{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.functional import F\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 3176\n",
    "target = 1024\n",
    "micro = 128\n",
    "B, L, H, D = 1, user+target, 3, 256\n",
    "\n",
    "# seq len: 3171 user length + 200 target length. ---> 3171 user length + 4096 target length\n",
    "\n",
    "q = torch.randn(B, L, H, D, dtype=torch.half, device=\"cuda\")\n",
    "k = torch.randn(B, L, H, D, dtype=torch.half, device=\"cuda\")\n",
    "v = torch.randn(B, L, H, D, dtype=torch.half, device=\"cuda\")\n",
    "## Experiment 1\n",
    "rab = torch.ones(B,1,L,L,dtype=torch.half, device=\"cuda\")\n",
    "def construct_mask():\n",
    "    user_matrix_len = 23 + 6 + 2 + 5 # 5 for padding \n",
    "    rt_seq_max_len = 100\n",
    "    rt_seq_length = 85\n",
    "    shopping_cart_seq_max_len = 40\n",
    "    shopping_cart_seq_length = 30\n",
    "\n",
    "    lifelong_seq_max_len = 3000\n",
    "    long_seq_length = 1024\n",
    "    target_seq_max_len = target\n",
    "    input_length = user_matrix_len + rt_seq_max_len + shopping_cart_seq_max_len + lifelong_seq_max_len + target_seq_max_len\n",
    "\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.half\n",
    "\n",
    "    # 计算attention mask\n",
    "    user_mask = torch.ones([1, input_length, user_matrix_len], dtype=torch.bool, device=device)\n",
    "\n",
    "    rt_attn_mask = torch.zeros([1, input_length, rt_seq_max_len], dtype=torch.bool, device=device)\n",
    "    # rt_autoregressive_mask = torch.triu(\n",
    "    #     torch.ones([1, rt_seq_length, rt_seq_length], dtype=torch.bool, device=device))\n",
    "    # rt_attn_mask[:, user_matrix_len:user_matrix_len + rt_seq_length, :rt_seq_length] = rt_autoregressive_mask\n",
    "    rt_autoregressive_mask = torch.arange(rt_seq_max_len, device=device)\n",
    "    rt_autoregressive_mask = rt_autoregressive_mask.view(1, rt_seq_max_len, 1) <= rt_autoregressive_mask.view(1, 1, rt_seq_max_len)\n",
    "    rt_valid_mask = (\n",
    "            torch.arange(rt_seq_max_len, device=device) < rt_seq_length).view(1, 1, rt_seq_max_len).expand(1, rt_seq_max_len, rt_seq_max_len)\n",
    "    rt_attn_mask[:, user_matrix_len:user_matrix_len+rt_seq_max_len, :] = torch.logical_and(rt_autoregressive_mask, rt_valid_mask)\n",
    "\n",
    "    rt_target_attn_mask = (\n",
    "            torch.arange(rt_seq_max_len, device=device) < rt_seq_length).view(1, 1, rt_seq_max_len).expand(1, target_seq_max_len, rt_seq_max_len)\n",
    "    rt_attn_mask[:, -target_seq_max_len:, :] = rt_target_attn_mask\n",
    "\n",
    "    shopping_attn_mask = (\n",
    "            torch.arange(shopping_cart_seq_max_len, device=device) < shopping_cart_seq_length).view(1, 1, shopping_cart_seq_max_len).expand(1, input_length, shopping_cart_seq_max_len)\n",
    "\n",
    "    lifelong_attn_mask = (\n",
    "            torch.arange(lifelong_seq_max_len, device=device) < long_seq_length).view(1, 1, lifelong_seq_max_len).expand(1, input_length, lifelong_seq_max_len)\n",
    "\n",
    "    target_attn_mask = torch.zeros([1, input_length, target_seq_max_len], dtype=torch.bool, device=device)\n",
    "    target_attn_mask[:, -target_seq_max_len:, :] = torch.diag(torch.ones(target_seq_max_len, dtype=torch.bool, device=device)).unsqueeze(dim=0)\n",
    "\n",
    "    invalid_attn_mask = torch.cat([user_mask, rt_attn_mask, shopping_attn_mask, lifelong_attn_mask, target_attn_mask], dim=-1)\n",
    "\n",
    "    return invalid_attn_mask\n",
    "\n",
    "mask = construct_mask().to(torch.half)\n",
    "\n",
    "def hstu_attention_v1(\n",
    "        q: torch.Tensor,\n",
    "        k: torch.Tensor,\n",
    "        v: torch.Tensor,\n",
    "        num_heads: int,\n",
    "        attention_dim: int,\n",
    "        linear_dim: int,\n",
    "        avg_factor: int,\n",
    "        invalid_attn_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    b, n, _ = q.size()\n",
    "    m = k.size(1)\n",
    "\n",
    "    #print(\"verbose {} {} {}\".format(b, n, m)) # 1 3371 3371\n",
    "    qk_attn = torch.einsum(\n",
    "        \"bnhd,bmhd->bhnm\",\n",
    "        q.reshape(b, n, num_heads, attention_dim),\n",
    "        k.reshape(b, m, num_heads, attention_dim),\n",
    "    )\n",
    "    qk_attn = qk_attn + rab.repeat(1,b,1,1)\n",
    "    # print(qk_attn.shape)            # torch.Size([1, 2, 3371, 3371])\n",
    "    qk_attn = F.silu(qk_attn) / avg_factor\n",
    "    # print(qk_attn.shape)            # torch.Size([1, 2, 3371, 3371])\n",
    "    qk_attn = qk_attn * invalid_attn_mask.unsqueeze(dim=1)\n",
    "    # print(qk_attn.shape)            # torch.Size([1, 2, 3371, 3371])\n",
    "    attn_output = torch.einsum(\n",
    "        \"bhnm,bmhd->bnhd\",\n",
    "        qk_attn,\n",
    "        v.reshape(b, m, num_heads, linear_dim)\n",
    "    ).reshape(b, n, num_heads * linear_dim)\n",
    "    # print(attn_output.shape)        # torch.Size([1, 3371, 512])\n",
    "    return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hstu_attention_v2():\n",
    "    \"\"\" pybind test nv hstu kernel\n",
    "    \"\"\"\n",
    "    from hstu import run_hstu\n",
    "    # from hstu_attn import hstu_attn_varlen_func\n",
    "    \n",
    "    cu_seqlens_q = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "    cu_seqlens_k = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "    \n",
    "    user_length = torch.tensor([user],device = \"cuda\")\n",
    "    target_length = torch.tensor([target],device = \"cuda\")\n",
    "    micro_bs = torch.tensor([micro], device=\"cuda\")\n",
    "\n",
    "\n",
    "    res = run_hstu(q.reshape(B * L, H, D).half(),\n",
    "            k.reshape(B * L, H, D).half(),\n",
    "            v.reshape(B * L, H, D).half(),cu_seqlens_q,cu_seqlens_k,user+target,\n",
    "            user+target,user_length,micro_bs,target_length,mask)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hstu_attention_v3():\n",
    "    \"\"\" pybind test nv hstu kernel\n",
    "    \"\"\"\n",
    "    from hstu import run_mask\n",
    "    # from hstu_attn import hstu_attn_varlen_func\n",
    "    \n",
    "    cu_seqlens_q = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "    cu_seqlens_k = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "\n",
    "    res = run_mask(q.reshape(B * L, H, D).half(),\n",
    "            k.reshape(B * L, H, D).half(),\n",
    "            v.reshape(B * L, H, D).half(),cu_seqlens_q,cu_seqlens_k,user+target,user+target,mask)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mask_rab():\n",
    "    from hstu import run_mask_rab\n",
    "    # from hstu_attn import hstu_attn_varlen_func\n",
    "    \n",
    "    cu_seqlens_q = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "    cu_seqlens_k = torch.tensor([0, L], dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "\n",
    "    res = run_mask_rab(q.reshape(B * L, H, D).half(),\n",
    "            k.reshape(B * L, H, D).half(),\n",
    "            v.reshape(B * L, H, D).half(),cu_seqlens_q,cu_seqlens_k,user+target,user+target,mask,rab)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_diff():\n",
    "    o1 = hstu_attention_v1(q.reshape(B, L, H * D),\n",
    "        k.reshape(B, L, H * D),\n",
    "        v.reshape(B, L, H * D),\n",
    "        H, \n",
    "        D,\n",
    "        D,\n",
    "        L,\n",
    "        mask).reshape(-1).to(device='cpu')\n",
    "    o2 = test_hstu_attention_v2().reshape(-1).to(device='cpu')\n",
    "    o3 = test_hstu_attention_v3().reshape(-1).to(device='cpu')\n",
    "    o4 = test_mask_rab().reshape(-1).to(device='cpu')\n",
    "    print(o1)\n",
    "    print(o2)\n",
    "    print(o3)\n",
    "    print(o4)\n",
    "\n",
    "    difference = o1.numpy() - o4.numpy()\n",
    "\n",
    "    print(f\"50 percentile : {np.percentile(difference, 50)}\")\n",
    "    print(f\"80 percentile : {np.percentile(difference, 80)}\")\n",
    "    print(f\"90 percentile : {np.percentile(difference, 90)}\")\n",
    "    print(f\"99 percentile : {np.percentile(difference, 99)}\")\n",
    "    pass\n",
    "\n",
    "    return o1,o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0812,  0.0423, -0.0554,  ...,  0.0789, -0.0472, -0.0142],\n",
      "       dtype=torch.float16)\n",
      "tensor([ 0.0773,  0.0404, -0.0517,  ...,  0.0757, -0.0450, -0.0097],\n",
      "       dtype=torch.float16)\n",
      "tensor([ 0.0773,  0.0404, -0.0517,  ...,  0.0757, -0.0450, -0.0097],\n",
      "       dtype=torch.float16)\n",
      "tensor([ 0.0812,  0.0423, -0.0554,  ...,  0.0788, -0.0472, -0.0142],\n",
      "       dtype=torch.float16)\n",
      "50 percentile : 0.0\n",
      "80 percentile : 1.52587890625e-05\n",
      "90 percentile : 3.0517578125e-05\n",
      "99 percentile : 0.0001220703125\n"
     ]
    }
   ],
   "source": [
    "o1,o2 = test_diff()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
